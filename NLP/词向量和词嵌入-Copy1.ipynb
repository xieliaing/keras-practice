{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量与词嵌入\n",
    "\n",
    "本章介绍词嵌入（word embedding）方法。依次有如下内容：\n",
    "1. 概述\n",
    "2. 机器学习对于词的表示方法\n",
    "3. 神经网络语言模型（Neural Network Language Model）\n",
    "4. word2vec构造词向量\n",
    "5. 使用keras完成词向量的训练与可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机器学习对于词的表示方法\n",
    "\n",
    "在NLP建模中，最重要和最根本的任务就是在模型中对输入的单词进行表示，方便模型识别单词之间的相似性和差异性。词的表达主要有两大类：早期的NLP工作中，词的表示大多是基于基本符号（atomic symbol）；而在现代NLP中，更多的是以词向量（word vector）的方法来表示，从而使得词间相似性计算更加便捷。下面先简要介绍早期的词表示方法，再着重解释现代的词向量方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词向量方法\n",
    "\n",
    "在真实世界中，单词的数量是巨大的。比如根据现代汉语词典，中文里常用的基本词和词组大约有5万6千多个，加上现在网络上新兴的各种单词及其组合，则无人有过精确统计；至于英语，韦伯斯特词典则大约收录了1百万个英语单词。当然，汉语的词数量不适宜跟英语相比，中文的词主要是一个词根作用，加上其各种组合应该数量也是巨大的。\n",
    "\n",
    "这些单词之间并不是完全独立的，也不是每一个单词都代表一个完全独立的概念。我们可以认为我们在谈话时所表达的实际语义实际更少，是在一个数量更低的维度上。比如性别（男 vs 女）；交通工具（汽车 vs 飞机），食品（宫保鸡丁 vs 火锅）等等。\n",
    "\n",
    "我们下面介绍几种常见的词向量表示方法：\n",
    "1. 独热编码（One Hot Encoding）\n",
    "2. 基于SVD的编码\n",
    "3. 迭代编码方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 独热编码 （One Hot Encoding）\n",
    "\n",
    "独热编码是对于当前单词表中的单词使用一个向量进行表达的简单方式。假如当前的单词表有10个不同的单词，那么独热编码使用10位的0、1数字来对该单词表进行编码，出现每个单词在对应的序号位标为1，否则为0。下面举例说明：\n",
    "\n",
    "假设我们的文档分词后产生如下的单词表：[“中国”，“国家”，“主席”，“习近平”，“北京”，“钓鱼台”，“会见”，“到”，“访”，“日本”，“首相”]，共11个单词，并且单词序号也按照上面的次序，那么我们的独热编码$w$就使用一个11个0、1数字的向量来表示这些单词：\n",
    "\n",
    "w(“中国”)：1000000000； w(“国家”)：01000000000，......，w(“首相”)：00000000001\n",
    "\n",
    "一般说来，对于一个具有N个元素的单词表，独热编码将每一个单词映射为$R^{1\\times|N|}$的向量，向量中对应单词序号的位置数值为1，其余为0。\n",
    "\n",
    "当然，独热编码虽然简单，但是其有几个问题：\n",
    "1. 存储效率极低\n",
    "2. 每个单词是完全独立的存在，之间并无联系。比如$w （“中国”）^T w（“日本”）=0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于SVD的编码\n",
    "我们看到，独热编码虽然简单，但是对于建模来说并不是较好的选择。研究者探索了可以解决存储效率和单词之间关系问题的方法，其中基于SVD的编码便是较常见的一种。这种方法依赖于通过word-document矩阵生成的共生矩阵（cooccurance matrix），下面分别介绍：\n",
    "1. 词-文矩阵（word-document matrix）。这是以单词为行，以文本为列的矩阵，反映一个文本中以0、1编码表示的所有单词；\n",
    "2. 基于滑动窗的共生矩阵（window based co-occurance matrix）。该矩阵是一个“词-词矩阵”（word-word matrix），反映的是在一个给定的窗口中两个单词同时出现的频次统计；\n",
    "\n",
    "直接按照逻辑来实现对以上矩阵的生产并不难，但是sklearn里面已经有现成的方法可供调用，直接对以列表形式出现的文本集合进行操作，非常方便。下面先举一个简单示例说明逻辑，再展示实践中可用的生产性代码。在英文文档中，可以直接使用sklearn的文字处理模块进行操作，但是中文需要先进行分词再使用sklearn的模块进行处理。\n",
    "\n",
    "下面，我们：\n",
    "- 首先展示如何生成**词-文矩阵**；\n",
    "- 再展示如何生成基于滑动窗的**共生矩阵**；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'cat', 'dog', 'fight', 'good', 'is', 'this']\n",
      "[[1 0 0 0 1 1 1]\n",
      " [0 1 0 0 1 1 2]\n",
      " [0 1 1 1 0 0 0]]\n",
      "{'this': 6, 'is': 5, 'good': 4, 'book': 0, 'cat': 1, 'dog': 2, 'fight': 3}\n"
     ]
    }
   ],
   "source": [
    "# 对于英文文档列表，可以直接使用sklearn工具生成 WORD-DOCUMENT Matrix \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "docs = ['this is a good book',\n",
    "        'this cat is this good',\n",
    "        'cat dog fight']\n",
    "count_model = CountVectorizer(ngram_range=(1,1)) # default unigram model\n",
    "X = count_model.fit_transform(docs)\n",
    "print(count_model.get_feature_names())\n",
    "print(X.todense())\n",
    "print(count_model.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对中文进行“词-文矩阵”生成**\n",
    "\n",
    "英文没有分词的要求，所有单词由空格自然分开；但是中文由于其特殊性，需要进行额外分词。这里我们使用流行的“结巴分词”（jieba）软件包来进行分词，读者也可以选择自己的分词软件。\n",
    "\n",
    "从上面的英文例子我们看到，对于中文文本，只需要将连续的中文使用jieba进行分词，将单词用空格隔开后放入一个字符串即可使用sklearn的现成方法生成词-文矩阵。下面我们进行演示。第一步我们需要将原有中文字符串列表中的中文字符串进行分词，再用空格连接后形成新的字符串放回列表中。\n",
    "\n",
    "根据jieba分词库的自我介绍，其支持三种分词模式：\n",
    "- 精确模式，试图将句子最精确地切开，适合文本分析，也是最常用的。该模式对应jieba.cut方法，使用**cut_all=False**选项\n",
    "- 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快而且颗粒度非常高，但是不能解决歧义，比如“...川普通过...”会被切分为“川普”，“普通”等多个词，但是这里实际上只有“川普”这个词有意义。该模式也对应jieba.cut方法，使用**cut_all=True**选项\n",
    "- 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。该模式对应**jieba.cut_for_search**方法。\n",
    "\n",
    "在jieba3k中，以上两种方法都返回一个可迭代的生成器（generator）对象，可以使用for 循环来获取分割后的单词。jieba分词会将阿拉伯数字单独分割出来，如果需要将这些阿拉伯数字挑出来，可以使用bool(re.search('\\d+', word))来判断。\n",
    "\n",
    "在老版本的jieba.cut和jieba.cut_for_search各自对应一个返回列表的函数，分别是lcut 和lcut_for_search，其用法一样，只是返回数据类型是一个列表，但是在jieba3k中似乎没有这些方法了。\n",
    "\n",
    "jieba也自带一个Tokenizer对象，可新建一个自定义的分词器，使用不同的词典，这里不再详述。\n",
    "\n",
    "对于我们的应用来说，只需要选好一个分词模式，对于原有中文字符串列表进行迭代，对于每一个字符串分词后的单词用空格连接起来再放回列表即可，下面的代码演示了采用精确模式进行分词的应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\xieliang\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.881 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['独热 编码 是 对于 当前 单词表 中 的 单词 使用 1 个 向量 进行 表达 的 简单 方式 ， 独热 编码 有 自身 的 缺点 和 有点 。', '假如 当前 的 单词表 有 10 个 不同 的 单词 ， 并且 每个 单词 都 不 一样', '研究者 探索 了 可以 解决 存储 效率 和 单词 之间 关系 问题 的 方法', '独热 编码 虽然 简单']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "jieba.initialize()  \n",
    "# 简单示例\n",
    "documents = [u'独热编码是对于当前单词表中的单词使用1个向量进行表达的简单方式，独热编码有自身的缺点和有点。', \n",
    "             u'假如当前的单词表有10个不同的单词，并且每个单词都不一样', \n",
    "             u'研究者探索了可以解决存储效率和单词之间关系问题的方法', \n",
    "             u'独热编码虽然简单']\n",
    "\n",
    "documents_after = []\n",
    "\n",
    "# 第一步先将中文词切分开，每个原有中文字符串组成一个单独的列表\n",
    "documents_after = [[w for w in jieba.cut(s)] for s in documents]\n",
    "# 第二部用空格将单独列表中的中文词元素连接成一个带空格的字符串，从而可以模仿英文的操作\n",
    "documents_after = [ ' '.join(s) for s in documents_after]\n",
    "print(documents_after)\n",
    "print(len(documents_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面进行矩阵构建操作，这里我们使用类似处理英文的标准方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文分词后的文档：\n",
      "['独热 编码 是 对于 当前 单词表 中 的 单词 使用 1 个 向量 进行 表达 的 简单 方式 ， 独热 编码 有 自身 的 缺点 和 有点 。', '假如 当前 的 单词表 有 10 个 不同 的 单词 ， 并且 每个 单词 都 不 一样', '研究者 探索 了 可以 解决 存储 效率 和 单词 之间 关系 问题 的 方法', '独热 编码 虽然 简单']\n",
      "\n",
      "\n",
      "标注化（Tokenized）后的特征列表：\n",
      "['10', '一样', '不同', '之间', '使用', '假如', '关系', '单词', '单词表', '可以', '向量', '存储', '对于', '并且', '当前', '探索', '效率', '方式', '方法', '有点', '每个', '独热', '研究者', '简单', '编码', '缺点', '自身', '虽然', '表达', '解决', '进行', '问题']\n",
      "\n",
      "\n",
      "词-文矩阵：\n",
      "[[0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 2 0 1 2 1 1 0 1 0 1 0]\n",
      " [1 1 1 0 0 1 0 2 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0]]\n",
      "\n",
      "\n",
      "词-文矩阵对应的特征索引号（矩阵列的序号）：\n",
      "{'独热': 21, '编码': 24, '对于': 12, '当前': 14, '单词表': 8, '单词': 7, '使用': 4, '向量': 10, '进行': 30, '表达': 28, '简单': 23, '方式': 17, '自身': 26, '缺点': 25, '有点': 19, '假如': 5, '10': 0, '不同': 2, '并且': 13, '每个': 20, '一样': 1, '研究者': 22, '探索': 15, '可以': 9, '解决': 29, '存储': 11, '效率': 16, '之间': 3, '关系': 6, '问题': 31, '方法': 18, '虽然': 27}\n",
      "\n",
      "\n",
      "带标签的词-文矩阵:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>一样</th>\n",
       "      <th>不同</th>\n",
       "      <th>之间</th>\n",
       "      <th>使用</th>\n",
       "      <th>假如</th>\n",
       "      <th>关系</th>\n",
       "      <th>单词</th>\n",
       "      <th>单词表</th>\n",
       "      <th>可以</th>\n",
       "      <th>...</th>\n",
       "      <th>研究者</th>\n",
       "      <th>简单</th>\n",
       "      <th>编码</th>\n",
       "      <th>缺点</th>\n",
       "      <th>自身</th>\n",
       "      <th>虽然</th>\n",
       "      <th>表达</th>\n",
       "      <th>解决</th>\n",
       "      <th>进行</th>\n",
       "      <th>问题</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  一样  不同  之间  使用  假如  关系  单词  单词表  可以 ...  研究者  简单  编码  缺点  自身  虽然  表达  \\\n",
       "0   0   0   0   0   1   0   0   1    1   0 ...    0   1   2   1   1   0   1   \n",
       "1   1   1   1   0   0   1   0   2    1   0 ...    0   0   0   0   0   0   0   \n",
       "2   0   0   0   1   0   0   1   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "3   0   0   0   0   0   0   0   0    0   0 ...    0   1   1   0   0   1   0   \n",
       "\n",
       "   解决  进行  问题  \n",
       "0   0   1   0  \n",
       "1   0   0   0  \n",
       "2   1   0   1  \n",
       "3   0   0   0  \n",
       "\n",
       "[4 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(u'中文分词后的文档：')\n",
    "print(documents_after)\n",
    "print('\\n')\n",
    "\n",
    "# 再使用sklearn模块生成word-document矩阵\n",
    "cn_count_model = CountVectorizer(ngram_range=(1,1), lowercase=False) # default unigram model, 缺省要小写变换在中文中不适用，会报错\n",
    "cnX = cn_count_model.fit_transform(documents_after)\n",
    "\n",
    "print(u'标注化（Tokenized）后的特征列表：')\n",
    "print(cn_count_model.get_feature_names())\n",
    "print('\\n')\n",
    "\n",
    "print(u'词-文矩阵：')\n",
    "print(cnX.todense())\n",
    "print('\\n')\n",
    "\n",
    "print(u'词-文矩阵对应的特征索引号（矩阵列的序号）：')\n",
    "print(cn_count_model.vocabulary_)\n",
    "print('\\n')\n",
    "\n",
    "# 将词-文矩阵的列打上标签：\n",
    "voc_df=pd.DataFrame.from_dict(cn_count_model.vocabulary_, columns=['idx'], orient='index').sort_values(by=['idx'])\n",
    "cols = list(voc_df.index)\n",
    "print('带标签的词-文矩阵:')\n",
    "pd.DataFrame(cnX.todense(), columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现上述操作是可行的，因此可以将第一步的操作封装为一个函数供后续步骤使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['独热 编码 是 对于 当前 单词表 中 的 单词 使用 1 个 向量 进行 表达 的 简单 方式 ， 独热 编码 有 自身 的 缺点 和 有点 。']\n"
     ]
    }
   ],
   "source": [
    "documents2 = [u'独热编码是对于当前单词表中的单词使用1个向量进行表达的简单方式，独热编码有自身的缺点和有点。']\n",
    "documents2_after = [[w for w in jieba.cut(s)] for s in documents2]\n",
    "documents2_after = [ ' '.join(s) for s in documents2_after]\n",
    "print(documents2_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_string_to_words(documents):\n",
    "    # 第一步先将中文词切分开，每个原有中文字符串组成一个单独的列表\n",
    "    documents_after = [[w for w in jieba.cut(s)] for s in documents]\n",
    "    # 第二部用空格将单独列表中的中文词元素连接成一个带空格的字符串，从而可以模仿英文的操作\n",
    "    documents_after = [ ' '.join(s) for s in documents_after]\n",
    "    return documents_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是在实际工作中存在大量文本，我们会发现不同词的出现频率非常不同。这时候单纯使用词-文矩阵里面的频率计数到模型中会造成有偏差的结果。事实上，如果一个不常见的词在某一个文本中出现，那么其携带的信息量反而是非常高的。一种常见的变换方法就是TF-IDF（term-frequency inverse document frequency）。这个方法将高频词按照在所有文本中出现的频次进行降低权重的操作，从而来“突出”低频词的作用。tfidf使得文本相对更可比，计算文本之间的相似性的时候更有意义。\n",
    "\n",
    "TFIDF在sklearn里面有现成的工具进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '一样', '不同', '之间', '使用', '假如', '关系', '单词', '单词表', '可以', '向量', '存储', '对于', '并且', '当前', '探索', '效率', '方式', '方法', '有点', '每个', '独热', '研究者', '简单', '编码', '缺点', '自身', '虽然', '表达', '解决', '进行', '问题']\n",
      "(4, 32)\n",
      "带标签的词-文矩阵:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>一样</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>不同</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>之间</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>使用</th>\n",
       "      <td>0.248108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>假如</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>关系</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>单词</th>\n",
       "      <td>0.158364</td>\n",
       "      <td>0.428563</td>\n",
       "      <td>0.197854</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>单词表</th>\n",
       "      <td>0.195611</td>\n",
       "      <td>0.264680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>可以</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>向量</th>\n",
       "      <td>0.248108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>存储</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>对于</th>\n",
       "      <td>0.248108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>并且</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>当前</th>\n",
       "      <td>0.195611</td>\n",
       "      <td>0.264680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>探索</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>效率</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>方式</th>\n",
       "      <td>0.248108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>方法</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>有点</th>\n",
       "      <td>0.248108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>每个</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>独热</th>\n",
       "      <td>0.391223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>研究者</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>简单</th>\n",
       "      <td>0.195611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>编码</th>\n",
       "      <td>0.391223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>缺点</th>\n",
       "      <td>0.248108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>自身</th>\n",
       "      <td>0.248108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>虽然</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>表达</th>\n",
       "      <td>0.248108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>解决</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>进行</th>\n",
       "      <td>0.248108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>问题</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3\n",
       "10   0.000000  0.335714  0.000000  0.000000\n",
       "一样   0.000000  0.335714  0.000000  0.000000\n",
       "不同   0.000000  0.335714  0.000000  0.000000\n",
       "之间   0.000000  0.000000  0.309976  0.000000\n",
       "使用   0.248108  0.000000  0.000000  0.000000\n",
       "假如   0.000000  0.335714  0.000000  0.000000\n",
       "关系   0.000000  0.000000  0.309976  0.000000\n",
       "单词   0.158364  0.428563  0.197854  0.000000\n",
       "单词表  0.195611  0.264680  0.000000  0.000000\n",
       "可以   0.000000  0.000000  0.309976  0.000000\n",
       "向量   0.248108  0.000000  0.000000  0.000000\n",
       "存储   0.000000  0.000000  0.309976  0.000000\n",
       "对于   0.248108  0.000000  0.000000  0.000000\n",
       "并且   0.000000  0.335714  0.000000  0.000000\n",
       "当前   0.195611  0.264680  0.000000  0.000000\n",
       "探索   0.000000  0.000000  0.309976  0.000000\n",
       "效率   0.000000  0.000000  0.309976  0.000000\n",
       "方式   0.248108  0.000000  0.000000  0.000000\n",
       "方法   0.000000  0.000000  0.309976  0.000000\n",
       "有点   0.248108  0.000000  0.000000  0.000000\n",
       "每个   0.000000  0.335714  0.000000  0.000000\n",
       "独热   0.391223  0.000000  0.000000  0.465809\n",
       "研究者  0.000000  0.000000  0.309976  0.000000\n",
       "简单   0.195611  0.000000  0.000000  0.465809\n",
       "编码   0.391223  0.000000  0.000000  0.465809\n",
       "缺点   0.248108  0.000000  0.000000  0.000000\n",
       "自身   0.248108  0.000000  0.000000  0.000000\n",
       "虽然   0.000000  0.000000  0.000000  0.590819\n",
       "表达   0.248108  0.000000  0.000000  0.000000\n",
       "解决   0.000000  0.000000  0.309976  0.000000\n",
       "进行   0.248108  0.000000  0.000000  0.000000\n",
       "问题   0.000000  0.000000  0.309976  0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cn_tf_model = TfidfVectorizer()\n",
    "X = cn_tf_model.fit_transform(documents_after)\n",
    "print(cn_tf_model.get_feature_names())\n",
    "print(X.shape)\n",
    "\n",
    "voc_df=pd.DataFrame.from_dict(cn_tf_model.vocabulary_, columns=['idx'], orient='index').sort_values(by=['idx'])\n",
    "cols = list(voc_df.index)\n",
    "print('带标签的词-文矩阵:')\n",
    "Xdf=pd.DataFrame(X.todense(), columns=cols)\n",
    "# 现在行对应单词，列对应文本\n",
    "Xdf.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 滑动窗口的共生矩阵生成\n",
    "滑动窗口的共生矩阵因为包含了两个单词在一定相邻距离上同时出现的频次，因此反映了单词之间的相关程度。\n",
    "\n",
    "如果不考虑窗口大小，从而在全局考虑共生矩阵的生成的化，只需要使用词-文矩阵的矩阵乘积即可，下面展示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共生矩阵：\n",
      "Wall time: 0 ns\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 1]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 1 0 1]] \n",
      "\n",
      "词-文矩阵对应的特征索引号（矩阵列的序号）：\n",
      "{'独热': 21, '编码': 24, '对于': 12, '当前': 14, '单词表': 8, '单词': 7, '使用': 4, '向量': 10, '进行': 30, '表达': 28, '简单': 23, '方式': 17, '自身': 26, '缺点': 25, '有点': 19, '假如': 5, '10': 0, '不同': 2, '并且': 13, '每个': 20, '一样': 1, '研究者': 22, '探索': 15, '可以': 9, '解决': 29, '存储': 11, '效率': 16, '之间': 3, '关系': 6, '问题': 31, '方法': 18, '虽然': 27}\n",
      "\n",
      "\n",
      "带标签的共生矩阵:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>一样</th>\n",
       "      <th>不同</th>\n",
       "      <th>之间</th>\n",
       "      <th>使用</th>\n",
       "      <th>假如</th>\n",
       "      <th>关系</th>\n",
       "      <th>单词</th>\n",
       "      <th>单词表</th>\n",
       "      <th>可以</th>\n",
       "      <th>...</th>\n",
       "      <th>研究者</th>\n",
       "      <th>简单</th>\n",
       "      <th>编码</th>\n",
       "      <th>缺点</th>\n",
       "      <th>自身</th>\n",
       "      <th>虽然</th>\n",
       "      <th>表达</th>\n",
       "      <th>解决</th>\n",
       "      <th>进行</th>\n",
       "      <th>问题</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>一样</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>不同</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>之间</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>使用</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>假如</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>关系</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>单词</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>单词表</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>可以</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>向量</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>存储</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>对于</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>并且</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>当前</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>探索</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>效率</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>方式</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>方法</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>有点</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>每个</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>独热</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>研究者</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>简单</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>编码</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>缺点</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>自身</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>虽然</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>表达</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>解决</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>进行</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>问题</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     10  一样  不同  之间  使用  假如  关系  单词  单词表  可以 ...  研究者  简单  编码  缺点  自身  虽然  表达  \\\n",
       "10    1   1   1   0   0   1   0   2    1   0 ...    0   0   0   0   0   0   0   \n",
       "一样    1   1   1   0   0   1   0   2    1   0 ...    0   0   0   0   0   0   0   \n",
       "不同    1   1   1   0   0   1   0   2    1   0 ...    0   0   0   0   0   0   0   \n",
       "之间    0   0   0   1   0   0   1   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "使用    0   0   0   0   1   0   0   1    1   0 ...    0   1   2   1   1   0   1   \n",
       "假如    1   1   1   0   0   1   0   2    1   0 ...    0   0   0   0   0   0   0   \n",
       "关系    0   0   0   1   0   0   1   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "单词    2   2   2   1   1   2   1   6    3   1 ...    1   1   2   1   1   0   1   \n",
       "单词表   1   1   1   0   1   1   0   3    2   0 ...    0   1   2   1   1   0   1   \n",
       "可以    0   0   0   1   0   0   1   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "向量    0   0   0   0   1   0   0   1    1   0 ...    0   1   2   1   1   0   1   \n",
       "存储    0   0   0   1   0   0   1   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "对于    0   0   0   0   1   0   0   1    1   0 ...    0   1   2   1   1   0   1   \n",
       "并且    1   1   1   0   0   1   0   2    1   0 ...    0   0   0   0   0   0   0   \n",
       "当前    1   1   1   0   1   1   0   3    2   0 ...    0   1   2   1   1   0   1   \n",
       "探索    0   0   0   1   0   0   1   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "效率    0   0   0   1   0   0   1   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "方式    0   0   0   0   1   0   0   1    1   0 ...    0   1   2   1   1   0   1   \n",
       "方法    0   0   0   1   0   0   1   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "有点    0   0   0   0   1   0   0   1    1   0 ...    0   1   2   1   1   0   1   \n",
       "每个    1   1   1   0   0   1   0   2    1   0 ...    0   0   0   0   0   0   0   \n",
       "独热    0   0   0   0   2   0   0   2    2   0 ...    0   3   5   2   2   1   2   \n",
       "研究者   0   0   0   1   0   0   1   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "简单    0   0   0   0   1   0   0   1    1   0 ...    0   2   3   1   1   1   1   \n",
       "编码    0   0   0   0   2   0   0   2    2   0 ...    0   3   5   2   2   1   2   \n",
       "缺点    0   0   0   0   1   0   0   1    1   0 ...    0   1   2   1   1   0   1   \n",
       "自身    0   0   0   0   1   0   0   1    1   0 ...    0   1   2   1   1   0   1   \n",
       "虽然    0   0   0   0   0   0   0   0    0   0 ...    0   1   1   0   0   1   0   \n",
       "表达    0   0   0   0   1   0   0   1    1   0 ...    0   1   2   1   1   0   1   \n",
       "解决    0   0   0   1   0   0   1   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "进行    0   0   0   0   1   0   0   1    1   0 ...    0   1   2   1   1   0   1   \n",
       "问题    0   0   0   1   0   0   1   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "\n",
       "     解决  进行  问题  \n",
       "10    0   0   0  \n",
       "一样    0   0   0  \n",
       "不同    0   0   0  \n",
       "之间    1   0   1  \n",
       "使用    0   1   0  \n",
       "假如    0   0   0  \n",
       "关系    1   0   1  \n",
       "单词    1   1   1  \n",
       "单词表   0   1   0  \n",
       "可以    1   0   1  \n",
       "向量    0   1   0  \n",
       "存储    1   0   1  \n",
       "对于    0   1   0  \n",
       "并且    0   0   0  \n",
       "当前    0   1   0  \n",
       "探索    1   0   1  \n",
       "效率    1   0   1  \n",
       "方式    0   1   0  \n",
       "方法    1   0   1  \n",
       "有点    0   1   0  \n",
       "每个    0   0   0  \n",
       "独热    0   2   0  \n",
       "研究者   1   0   1  \n",
       "简单    0   1   0  \n",
       "编码    0   2   0  \n",
       "缺点    0   1   0  \n",
       "自身    0   1   0  \n",
       "虽然    0   0   0  \n",
       "表达    0   1   0  \n",
       "解决    1   0   1  \n",
       "进行    0   1   0  \n",
       "问题    1   0   1  \n",
       "\n",
       "[32 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [u'独热编码是对于当前单词表中的单词使用1个向量进行表达的简单方式，独热编码有自身的缺点和有点。', \n",
    "             u'假如当前的单词表有10个不同的单词，并且每个单词都不一样', \n",
    "             u'研究者探索了可以解决存储效率和单词之间关系问题的方法', \n",
    "             u'独热编码虽然简单']\n",
    "documents_after  = cn_string_to_words(documents)\n",
    "\n",
    "# 再使用sklearn模块生成word-document矩阵\n",
    "min_n = 1\n",
    "max_n = 1\n",
    "cn_count_model = CountVectorizer(ngram_range=(min_n, max_n), lowercase=False) # 缺省为 unigram 模型\n",
    "cnX = cn_count_model.fit_transform(documents_after)\n",
    "\n",
    "\n",
    "print(u'共生矩阵：')\n",
    "#cnXdense = cnX.todense()\n",
    "#%time XTX = np.dot(cnXdense.T, cnXdense)\n",
    "%time XTX = cnX.T * cnX\n",
    "print(XTX.todense(), '\\n')\n",
    "\n",
    "\n",
    "print(u'词-文矩阵对应的特征索引号（矩阵列的序号）：')\n",
    "print(cn_count_model.vocabulary_)\n",
    "print('\\n')\n",
    "\n",
    "# 将共生矩阵的列打上标签：\n",
    "voc_df=pd.DataFrame.from_dict(cn_count_model.vocabulary_, columns=['idx'], orient='index').sort_values(by=['idx'])\n",
    "cols = list(voc_df.index)\n",
    "print('带标签的共生矩阵:')\n",
    "pd.DataFrame(XTX.todense(), columns=cols, index=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要生成含有特定窗口宽度的共生矩阵，这时我们需要自己写一个移动窗口函数，对于任意给定的单词（以下标index表示），在其左右给定窗口范围内获取邻近词。该操作可以使用如下的list comprehension操作实现：\n",
    "start = index - window_size\n",
    "end = index + window_size + 1\n",
    "neighbor_words = [words[i] for i in range(start, end) if 0 <= i < L and i != index]\n",
    "\n",
    "其中，L是字符串长度。注意，这里我们的输入是已经转换为下标的，以空格间隔的中文单词字符串。\n",
    "\n",
    "下面，我们先使用一个双重list comprehension将中文单词依照单词表转换为下标。由于下标0在keras的所有文字处理方法中都是保留下标，程序不会认为该数字对应于一个单词，因此这里我们也将sklearn生成的下标值+1。\n",
    "\n",
    "其次再编写一个函数，使用移动窗口技巧获取临近单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22, 25, 13, 15, 9, 8, 5, 11, 31, 29, 24, 18, 22, 25, 27, 26, 20], [6, 15, 9, 1, 3, 8, 14, 21, 8, 2], [23, 16, 10, 30, 12, 17, 8, 4, 7, 32, 19], [22, 25, 28, 24]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = cn_count_model.vocabulary_\n",
    "V = len(vocabulary)\n",
    "documents_index = [[vocabulary[word]+1 for word in doc.split(' ') if word in vocabulary.keys()] for doc in documents_after]\n",
    "print(documents_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "def generate_cooccurancemat(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    result = np.zeros([V, V], dtype=int)    \n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            #获取index对应的近邻词列表\n",
    "            contexts = []                 \n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "            #对于该共生组合，对于其相应矩阵位置计数+1\n",
    "            for k in x[0]:\n",
    "                if k!=0:\n",
    "                    result[word-1, k-1] += 1                        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>一样</th>\n",
       "      <th>不同</th>\n",
       "      <th>之间</th>\n",
       "      <th>使用</th>\n",
       "      <th>假如</th>\n",
       "      <th>关系</th>\n",
       "      <th>单词</th>\n",
       "      <th>单词表</th>\n",
       "      <th>可以</th>\n",
       "      <th>...</th>\n",
       "      <th>研究者</th>\n",
       "      <th>简单</th>\n",
       "      <th>编码</th>\n",
       "      <th>缺点</th>\n",
       "      <th>自身</th>\n",
       "      <th>虽然</th>\n",
       "      <th>表达</th>\n",
       "      <th>解决</th>\n",
       "      <th>进行</th>\n",
       "      <th>问题</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>一样</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>不同</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>之间</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>使用</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>假如</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>关系</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>单词</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>单词表</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>可以</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>向量</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>存储</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>对于</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>并且</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>当前</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>探索</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>效率</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>方式</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>方法</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>有点</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>每个</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>独热</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>研究者</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>简单</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>编码</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>缺点</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>自身</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>虽然</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>表达</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>解决</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>进行</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>问题</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     10  一样  不同  之间  使用  假如  关系  单词  单词表  可以 ...  研究者  简单  编码  缺点  自身  虽然  表达  \\\n",
       "10    0   0   1   0   0   1   0   1    1   0 ...    0   0   0   0   0   0   0   \n",
       "一样    0   0   0   0   0   0   0   1    0   0 ...    0   0   0   0   0   0   0   \n",
       "不同    1   0   0   0   0   0   0   1    1   0 ...    0   0   0   0   0   0   0   \n",
       "之间    0   0   0   0   0   0   1   1    0   0 ...    0   0   0   0   0   0   0   \n",
       "使用    0   0   0   0   0   0   0   1    1   0 ...    0   0   0   0   0   0   1   \n",
       "假如    1   0   0   0   0   0   0   0    1   0 ...    0   0   0   0   0   0   0   \n",
       "关系    0   0   0   1   0   0   0   1    0   0 ...    0   0   0   0   0   0   0   \n",
       "单词    1   1   1   1   1   0   1   2    2   0 ...    0   0   0   0   0   0   0   \n",
       "单词表   1   0   1   0   1   1   0   2    0   0 ...    0   0   1   0   0   0   0   \n",
       "可以    0   0   0   0   0   0   0   0    0   0 ...    1   0   0   0   0   0   0   \n",
       "向量    0   0   0   0   1   0   0   1    1   0 ...    0   1   0   0   0   0   1   \n",
       "存储    0   0   0   1   0   0   0   1    0   1 ...    0   0   0   0   0   0   0   \n",
       "对于    0   0   0   0   0   0   0   1    1   0 ...    0   0   1   0   0   0   0   \n",
       "并且    1   1   1   0   0   0   0   2    0   0 ...    0   0   0   0   0   0   0   \n",
       "当前    1   0   1   0   1   1   0   1    2   0 ...    0   0   1   0   0   0   0   \n",
       "探索    0   0   0   0   0   0   0   0    0   1 ...    1   0   0   0   0   0   0   \n",
       "效率    0   0   0   1   0   0   1   1    0   1 ...    0   0   0   0   0   0   0   \n",
       "方式    0   0   0   0   0   0   0   0    0   0 ...    0   1   1   0   1   0   1   \n",
       "方法    0   0   0   1   0   0   1   0    0   0 ...    0   0   0   0   0   0   0   \n",
       "有点    0   0   0   0   0   0   0   0    0   0 ...    0   0   1   1   1   0   0   \n",
       "每个    0   1   1   0   0   0   0   2    0   0 ...    0   0   0   0   0   0   0   \n",
       "独热    0   0   0   0   0   0   0   0    0   0 ...    0   2   3   1   1   1   1   \n",
       "研究者   0   0   0   0   0   0   0   0    0   1 ...    0   0   0   0   0   0   0   \n",
       "简单    0   0   0   0   0   0   0   0    0   0 ...    0   0   2   0   0   1   1   \n",
       "编码    0   0   0   0   0   0   0   0    1   0 ...    0   2   0   1   1   1   0   \n",
       "缺点    0   0   0   0   0   0   0   0    0   0 ...    0   0   1   0   1   0   0   \n",
       "自身    0   0   0   0   0   0   0   0    0   0 ...    0   0   1   1   0   0   0   \n",
       "虽然    0   0   0   0   0   0   0   0    0   0 ...    0   1   1   0   0   0   0   \n",
       "表达    0   0   0   0   1   0   0   0    0   0 ...    0   1   0   0   0   0   0   \n",
       "解决    0   0   0   0   0   0   0   1    0   1 ...    1   0   0   0   0   0   0   \n",
       "进行    0   0   0   0   1   0   0   1    0   0 ...    0   1   0   0   0   0   1   \n",
       "问题    0   0   0   1   0   0   1   1    0   0 ...    0   0   0   0   0   0   0   \n",
       "\n",
       "     解决  进行  问题  \n",
       "10    0   0   0  \n",
       "一样    0   0   0  \n",
       "不同    0   0   0  \n",
       "之间    0   0   1  \n",
       "使用    0   1   0  \n",
       "假如    0   0   0  \n",
       "关系    0   0   1  \n",
       "单词    1   1   1  \n",
       "单词表   0   0   0  \n",
       "可以    1   0   0  \n",
       "向量    0   1   0  \n",
       "存储    1   0   0  \n",
       "对于    0   0   0  \n",
       "并且    0   0   0  \n",
       "当前    0   0   0  \n",
       "探索    1   0   0  \n",
       "效率    1   0   0  \n",
       "方式    0   1   0  \n",
       "方法    0   0   1  \n",
       "有点    0   0   0  \n",
       "每个    0   0   0  \n",
       "独热    0   0   0  \n",
       "研究者   1   0   0  \n",
       "简单    0   1   0  \n",
       "编码    0   0   0  \n",
       "缺点    0   0   0  \n",
       "自身    0   0   0  \n",
       "虽然    0   0   0  \n",
       "表达    0   1   0  \n",
       "解决    0   0   0  \n",
       "进行    0   0   0  \n",
       "问题    0   0   0  \n",
       "\n",
       "[32 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurancemat = generate_cooccurancemat(documents_index, 3, V)\n",
    "pd.DataFrame(cooccurancemat, columns=cols, index=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中因为信息量较小，两个词同时出现多次的情况没有，因此这里使用较大的外部数据来进行展示。我们使用的是1000条酒店评论数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['除非', '除了', '此', '此间', '此外', '从', '从而', '打', '待', '但', '但是', '当', '当着', '到', '得', '的', '的话']\n",
      "['独热编码是对于当前单词表中的单词使用1个向量进行表达的简单方式，独热编码有自身的缺点和有点。', '假如当前的单词表有10个不同的单词，并且每个单词都不一样', '研究者探索了可以解决存储效率和单词之间关系问题的方法', '独热编码虽然简单']\n"
     ]
    }
   ],
   "source": [
    "def load_hotel_data():\n",
    "    documents2 = []\n",
    "    stopword = []\n",
    "    datafile_pos = './nlp_data/hotel_reviews_data/1000_pos.txt'\n",
    "    datafile_neg = './nlp_data/hotel_reviews_data/1000_neg.txt'\n",
    "    stopwordfile = './nlp_data/hotel_reviews_data/stopWord.txt'\n",
    "\n",
    "    # 先读入stopword\n",
    "    #fo=open(stopwordfile, encoding='UTF-8')\n",
    "    with open(stopwordfile, encoding='UTF-8') as fo:\n",
    "        for line in fo:\n",
    "           stopword.append(line.strip('\\n'))\n",
    "\n",
    "    print(stopword[70:87])\n",
    "\n",
    "    # 再读入原始评论文档\n",
    "    with open(datafile_pos, encoding='UTF-8') as fo:\n",
    "        for line in fo:\n",
    "           documents2.append(line.strip('\\n'))\n",
    "\n",
    "    with open(datafile_neg, encoding='UTF-8') as fo:\n",
    "        for line in fo:\n",
    "           documents2.append(line.strip('\\n'))    \n",
    "    return documents, stopword\n",
    "\n",
    "documents2, stopwords = load_hotel_data();\n",
    "print(documents2[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果滑动窗口大小设为1，则考虑全局的共生矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopword' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a67b7646a90b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmin_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmax_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcn_count_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mcnX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcn_count_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments2_after\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopword' is not defined"
     ]
    }
   ],
   "source": [
    "# 首先分词\n",
    "documents2_after = cn_string_to_words(documents2)\n",
    "\n",
    "# 建立词表索引，用来将文字转换为下标，从而我们可以使用上面的generate_cooccurancemat函数\n",
    "min_n = 1\n",
    "max_n = 1\n",
    "cn_count_model = CountVectorizer(ngram_range=(min_n, max_n), lowercase=False, stop_words = stopword)\n",
    "cnX = cn_count_model.fit_transform(documents2_after)\n",
    "\n",
    "vocabulary = cn_count_model.vocabulary_\n",
    "# 考虑到阿拉伯数字没有多少意义，这里我们要先将阿拉伯数字从单词表中剔除。首先制作一个词典索引拷贝列表，\n",
    "#其中只保留阿拉伯数字元素，再在原始词典中逐一pop出阿拉伯数字\n",
    "#bool(re.search('\\d+', word))\n",
    "keys = list(vocabulary.keys())\n",
    "vocabulary2 = vocabulary.copy()\n",
    "numkeys = [k for k in keys if bool(re.search('\\d+', k))]\n",
    "for key in numkeys:\n",
    "    vocabulary2.pop(key)\n",
    "\n",
    "# 清洁工作\n",
    "del(keys)\n",
    "# 重新fit数据，但是numkeys会被添加到stopword里\n",
    "stopword.extend(numkeys)\n",
    "cn_count_model2 = CountVectorizer(ngram_range=(min_n, max_n), lowercase=False, stop_words = stopword)\n",
    "cnX = cn_count_model2.fit_transform(documents2_after) \n",
    "\n",
    "vocabulary2 = cn_count_model2.vocabulary_\n",
    "documents_index = [[vocabulary2[word]+1 for word in doc.split(' ') if word in vocabulary2.keys()] for doc in documents2_after]\n",
    "print(documents_index[:3])\n",
    "\n",
    "# 现在开始使用generate_cooccurancemat函数生成共生矩阵\n",
    "print(u'使用generate_cooccurancemat函数生成共生矩阵：')\n",
    "XTX = generate_cooccurancemat(documents_index, 10, len(vocabulary2.keys()))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# 将共生矩阵的列打上标签：\n",
    "voc_df=pd.DataFrame.from_dict(vocabulary2, columns=['idx'], orient='index').sort_values(by=['idx'])\n",
    "cols = list(voc_df.index)\n",
    "print('带标签的词-文矩阵:')\n",
    "pd.DataFrame(XTX, columns=cols, index=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将共生矩阵进行SVD分解，获取每个单词在新空间上的映射向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=10, n_iter=10, random_state=42)\n",
    "%time svd.fit(XTX)\n",
    "u = svd.transform(XTX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(u[:, 0], u[:, 1], s=2)\n",
    "plt.xlim(-2.25, 3.25)\n",
    "plt.ylim(-2.25, 2.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 这里结束传统的基于词频的表达方法，开始介绍基于预测的表达方法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 迭代嵌入方法\n",
    "我们在上面介绍了基于SVD的词嵌入方法。通过共生矩阵以及SVD算法，独热法表示的单词之间的关系得到抽象，并映射到较低维度的致密空间中。但是这种方法基于全局信息，对存储量需求大。我们现在介绍一种迭代学习的方法来将单词映射到一个新的包含了上下文关系的空间中。这种方法的典型代表叫word2vec。在word2vec这个方法中需要引入两个概念“中心词”（center word）和“上下文”（context）\n",
    "\n",
    "以“资产富裕的人爱投资股票”这句话为例，其经过处理后得到如下的单词：\\[ '资产','富裕'，'人', '爱', '投资', '股票' \\]。如果选定*‘富裕’*这个单词，在模型中其被称为“中心词”，而其上下文就是前后的\\[ '资产','人', '爱', '投资', '股票' \\]。一个中心词的上下文被word2vec这个算法用来衡量其含义。如果我们在大量的文本中都发现类似于“资产”，“财富”，“投资”这样的词经常性地出现在*“富裕”*这个词的周边，就能推断*“富裕”*的含义。有相似上下文的词就在word2vec这个模型里具备相似的含义，可以被视作是同义词，其对应的词向量则应该距离上接近。\n",
    "\n",
    "当然，在实践中，上下文通常被定义为中心词左右对称给定长度的窗口覆盖的词，如下图所示：\n",
    "\n",
    "<img src=\"./pics/Chapter1-1.png\" width=\"400\">\n",
    "\"中心词与上下文的独热表示\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在word2vec中，词向量本身就是模型的参数，通过对数据的建模可以获得参数的值。word2vec包含两种不同的子模型：\n",
    "1. 连续型模型（Continuous Bag Of Words Model, CBOW Model）\n",
    "2. 跳跃型模型（Skip-Gram Model）\n",
    "\n",
    "下面分别介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 连续型模型 （CBOW） **\n",
    "\n",
    "在CBOW中，模型是根据上下文单词来预测中心词。其基本步骤如下：\n",
    "\n",
    "\n",
    "其架构如下图所示（图源自于：\"Deep learning for sentence classification\"）。图中标识分别为：\n",
    "1. $x_{ik}$为独热表示的上下文单词\n",
    "2. $W_{V \\times N}$为输入层的权重矩阵，大小为$V \\times N$，其中V是词典大小，而N是设定的词向量维度\n",
    "3. $h_i$是由$x_{ik}$与权重矩阵$W$相乘的到的隐含层结果，大小为$1 \\times N$的向量\n",
    "4. $W^{'}_{N \\times V}$为输出层的权重矩阵。注意，这里的$W^{'}$不是$W$矩阵的转置，而是另外一个全新的矩阵。\n",
    "5. $y_j$为待预测的中心词的独热表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./pics/Chapter1-CBOW.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW的计算过程比较直接：\n",
    "\n",
    "1. 首先，对于给定窗口长度m，上下文$x^{(c)}$用相应的独热法表示，对于待预测的中心词$y^{(c)}$也用相应的独热编码表示，那么我们的输入数据为：$(x^{(c-m)},...,x^{(c-1)}, x^{(c+1)},..., x^{(c+m)} )$\n",
    "2. 其次，对于上述上下文单词，与输入层权重矩阵相乘，得到嵌入向量$v_{c-m} = Wx^{(c-m)}, v_{c-m+1} = Wx^{(c-m+1)}, ....$\n",
    "3. 将上述嵌入向量取平均：$\\bar(v) = \\frac{\\sum_{-m}^{m}v_{c+j}}{2m}$\n",
    "4. 将平均的嵌入向量与输出层权重矩阵相乘，得到输出打分向量$z = W{'}\\bar{v}$。相似的单词该得分应该更高；\n",
    "5. 应用softmax函数将上述打分向量变为概率输出$y=\\textrm{softmax}(z)$。如果预测准确的话，那么概率向量$y$会在独热编码为1的地方具备最高的概率值。\n",
    "\n",
    "word2vec模型就是要在迭代的过程中，通过不断优化$W$和$W'$两个权重矩阵使得我们的语言模型尽可能地接近实际的数据表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个完整的CBOW模型的keras实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from IPython.display import SVG, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocabulary2)\n",
    "embedding_dim = 50\n",
    "window_size = 5\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=V, output_dim=embedding_dim, input_length=window_size*2, name='Embedding'))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,), name='Averaging'))\n",
    "cbow.add(Dense(V, activation='softmax', name='Output'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "display(SVG(model_to_dot(cbow, show_shapes=True).create(prog='dot', format='svg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 跳跃型模型 （Skip-Gram） **\n",
    "\n",
    "Skip-Gram模型与CBOW正好相反，模型是根据中心词来预测上下文。其基本步骤如下：\n",
    "\n",
    "\n",
    "其架构如下图所示（图源自于：\"Deep learning for sentence classification\"）。图中标识分别为：\n",
    "1. $x_{ik}$为独热表示的中心词\n",
    "2. $W_{V \\times N}$为输入层的权重矩阵，大小为$V \\times N$，其中V是词典大小，而N是设定的词向量维度\n",
    "3. $h_i$是由$x_{ik}$与权重矩阵$W$相乘的到的隐含层结果，大小为$1 \\times N$的向量\n",
    "4. $W^{'}_{N \\times V}$为输出层的权重矩阵。注意，这里的$W^{'}$不是$W$矩阵的转置，而是另外一个全新的矩阵。\n",
    "5. $y_{Cj}$为待预测的上下文词的独热表示\n",
    "\n",
    "<img src=\"./pics/Chapter1-SkipGram.png\" width=\"400\">\n",
    "\n",
    "\n",
    "Initialization\n",
    "Step 1\n",
    "\n",
    "    Fix the hyperparameters such as window size (2 * n + 1), the size of feature vectors (N), learning rate, number of epochs etc.\n",
    "    Initialize other parameters such as the connection weights between the input layer and hidden layer, and hidden layer and the softmax layers (can be random initialization).\n",
    "\n",
    "Step 2\n",
    "\n",
    "From each sentence in the input file, form a multi-set which will be a collection of every possible contiguous sub-sequence of size (2 * n + 1). Union of all such multi-sets (corresponding to each sentence) will be our training corpus (which is also a multi-set of windows).\n",
    "Step 3\n",
    "\n",
    "Make a vocabulary by collecting all possible distinct words. Let the Vocabulary be V.\n",
    "Step 4 [redundant step added for clarity in understanding]\n",
    "\n",
    "To each word in dictionary assign a random feature vector. If x is the word then Vec[x] represents the feature vector. The ith row of weight matrix between the input layer and hidden layer stores the feature vector for ith word in the vocabulary, hence in step 1 we have already initialized the word vectors for each word in the dictionary.\n",
    "One Training Step\n",
    "\n",
    "Input: Given a single window  w-n  …  w-1  x  w1  ….  wn  from the training corpus.\n",
    "Forward Step\n",
    "\n",
    " \n",
    "\n",
    "Input Layer\n",
    "\n",
    "    Activation Function: Identity\n",
    "    Input: One hot representation of word x\n",
    "    Weight Matrix Size: [V * N]\n",
    "    Objective: Select vector Vec[x] corresponding to x\n",
    "\n",
    "By feeding one hot representation of word x, the input vector to the hidden layer will be the feature vector corresponding to the word x.\n",
    "\n",
    "Note: The ith row of the hidden weight matrix is the word vector of the ith word in the vocabulary.\n",
    "\n",
    " \n",
    "Hidden Layer\n",
    "\n",
    "    Activation Function: Identity\n",
    "    Input: Word vector corresponding to x\n",
    "    Weight Matrix Size: [N * V] (for each of 2 * n softmax unit)\n",
    "\n",
    "There are 2 * n softmax units (each having |V| nodes) in the output layer. Let the softmax units be S-n … S-1 ,S1  …. Sn. Feed Vec[x] to each of the softmax unit (each with its own weight matrix). zth node of softmax unit i tries to predict the probability of zth word in dictionary appearing in context position i with respect to the current context. Let P(i, z) be the output of zth node of the ith softmax unit.\n",
    "\n",
    " \n",
    "Output Layer\n",
    "\n",
    "The following bullet points are with respect to a single softmax unit (say ith) unit in the output layer:\n",
    "\n",
    "    Input : Output of hidden layer.\n",
    "    Objective (Redundant): zth node of softmax unit i tries to predict the probability of zth word in dictionary appearing in context position i with respect to the current context.\n",
    "\n",
    "Crucial Step: For each context position in set P, add the log of the output of each corresponding context word’s node. So let S = $latex \\sum_{i = -n, i \\neq 0}^{i = n} \\log P(i, wi) $ where  wi is the  ith context word and i ∈ P (Objective here is to maximize this log likelihood of the observation).\n",
    "Backward Step (Back-Propagation Step):\n",
    "\n",
    "Objective: Try to maximize S (log likelihood of observation as defined above).\n",
    "\n",
    "S is a function of weights of the model, all words in vocabulary and the word vector corresponding to x. Using back propagation we tune weights of the model (specifically all weight between hidden layer and output layer) and improve the feature vectors word x (which is also embodied in model in the form of weight between input layer and hidden layer), so as to maximize S. This is same as minimizing the negative log likelihood of the observation.\n",
    "\n",
    "Redundant Note: During one training step, in a window where x forms the middle word, only the weights between hidden layer and output layer, and feature vector of x will be improved. Feature vectors of all other words will not change (Do some math yourself :p). From the next training instance, the modified feature vector of x will be used as its feature vector. In this way, a feature vector gets incrementally.\n",
    "Training\n",
    "\n",
    "Run the training step on the training corpus for several epochs.\n",
    "Output VS Outcome\n",
    "\n",
    "Output (of the model): Given a window (a training instance), we get the output as the values which we get at the output nodes. The output of the nodes (of output layer) tries to estimate probabilities. These probabilities can be useful for language modeling, though it is generally not used.\n",
    "\n",
    "Outcome: After the model is trained completely, the  ith rows of the weight matrix between input layer and hidden layer is the feature vector for the ith word in the dictionary. These feature vectors for each word in dictionary are the outcome of the model. And Word2vec is famous good feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文字符预处理：\n",
    "\n",
    "如前所述在我们编写自己的CBOW或者SkipGram模型之前，我们先要对中文字符进行处理。虽然前面我们已经有了一些方法来处理中文，但是我们想能不能将中文处理纳入keras的文字预处理模块来整体考虑？下面我们将对keras的文字预处理模块进行改造，使其标注类（Tokenizer）能顺利处理中文。我们将从一下几个方面着手。\n",
    "\n",
    "1. 标注化（tokenizer）\n",
    "2. 序列化\n",
    "3. 建立字典\n",
    "4. 建立正向、反向查阅表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "def cntext_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',):\n",
    "    translate_dict = dict((c, ' ') for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    words = jieba.cut(text)\n",
    "    seq = [w for w in words if w]\n",
    "    return seq\n",
    "\n",
    "def fit_on_cntexts(texts, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    '''\n",
    "    假设输入是一组中文列表\n",
    "    '''\n",
    "    index_docs = defaultdict(int)\n",
    "    word_docs = defaultdict(int)\n",
    "    word_counts = OrderedDict()\n",
    "    word_index = dict()\n",
    "    index_word = dict()\n",
    "    document_count = 0\n",
    "    for text in texts:\n",
    "        document_count += 1\n",
    "        #rint(text)\n",
    "        \n",
    "        if isinstance(text, list):\n",
    "            longtext = ' '.join(text)           \n",
    "            text = longtext\n",
    "        seq = cntext_to_word_sequence(text, filters)\n",
    "        for w in seq:\n",
    "            if w in word_counts:\n",
    "                word_counts[w] += 1\n",
    "            else:\n",
    "                word_counts[w] = 1\n",
    "        for w in set(seq):\n",
    "        # In how many documents each word occurs\n",
    "           word_docs[w] += 1\n",
    "\n",
    "        wcounts = list(word_counts.items())\n",
    "        wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "        # forcing the oov_token to index 1 if it exists        \n",
    "        if oov_token ==None:\n",
    "            sorted_voc = []\n",
    "        else:\n",
    "            sorted_voc = [oov_token]\n",
    "        sorted_voc.extend(wc[0] for wc in wcounts)\n",
    "        #rint(sorted_voc)\n",
    "\n",
    "        # note that index 0 is reserved, never assigned to an existing word\n",
    "        word_index = dict(\n",
    "            list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1))))\n",
    "        )\n",
    "\n",
    "        index_word = dict((c, w) for w, c in word_index.items())\n",
    "\n",
    "        for w, c in list(word_docs.items()):\n",
    "            index_docs[word_index[w]] = c\n",
    "\n",
    "    return seq, word_index, index_word, word_docs, index_docs, word_counts, document_count\n",
    "    \n",
    "def cntexts_to_sequences(texts, num_words, oov_token, document_count, filters, word_index ):\n",
    "    \"\"\"Transforms each text in texts to a sequence of integers.\n",
    "    Only top `num_words-1` most frequent words will be taken into account.\n",
    "    Only words known by the tokenizer will be taken into account.\n",
    "    # Arguments\n",
    "        texts: A list of texts (strings).\n",
    "    # Returns\n",
    "        A list of sequences.\n",
    "    \"\"\"\n",
    "    return list(cntexts_to_sequences_generator(texts, num_words, oov_token, document_count, filters, word_index))    \n",
    "\n",
    "\n",
    "def cntexts_to_sequences_generator(texts, num_words, oov_token, document_count, filters, word_index ):\n",
    "    \"\"\"Transforms each Chinese text in `texts` to a sequence of integers.\n",
    "    Each item in texts can also be a list,\n",
    "    in which case we assume each item of that list to be a token.\n",
    "    Only top `num_words-1` most frequent words will be taken into account.\n",
    "    Only words known by the tokenizer will be taken into account.\n",
    "    # Arguments\n",
    "        texts: A list of texts (strings).\n",
    "    # Yields\n",
    "        Yields individual sequences.\n",
    "    \"\"\"\n",
    "    num_words = num_words\n",
    "    oov_token_index = word_index.get(oov_token)\n",
    "    for text in texts:\n",
    "        document_count += 1\n",
    "        #print(text)\n",
    "\n",
    "        if isinstance(text, list):\n",
    "            longtext = ' '.join(text)           \n",
    "            text = longtext\n",
    "        seq = cntext_to_word_sequence(text, filters)\n",
    "        #print(seq, '\\n')\n",
    "                \n",
    "        vect = []\n",
    "        temp = []\n",
    "        for w in seq:\n",
    "            i = word_index.get(w)\n",
    "            temp.append(i)\n",
    "            if i is not None:\n",
    "                if num_words and i >= num_words:\n",
    "                    if oov_token_index is not None:\n",
    "                        vect.append(oov_token_index)\n",
    "                else:\n",
    "                    vect.append(i)\n",
    "            elif oov_token is not None:\n",
    "                vect.append(oov_token_index)\n",
    "        #print(temp)\n",
    "        #print('-------------')\n",
    "        yield vect       \n",
    "\n",
    "        \n",
    "class Tokenizer(object):\n",
    "    \"\"\"Text tokenization utility class.\n",
    "    This class allows to vectorize a text corpus, by turning each\n",
    "    text into either a sequence of integers (each integer being the index\n",
    "    of a token in a dictionary) or into a vector where the coefficient\n",
    "    for each token could be binary, based on word count, based on tf-idf...\n",
    "    # Arguments\n",
    "        num_words: the maximum number of words to keep, based\n",
    "            on word frequency. Only the most common `num_words-1` words will\n",
    "            be kept.\n",
    "        filters: a string where each element is a character that will be\n",
    "            filtered from the texts. The default is all punctuation, plus\n",
    "            tabs and line breaks, minus the `'` character.\n",
    "        lower: boolean. Whether to convert the texts to lowercase.\n",
    "        split: str. Separator for word splitting.\n",
    "        char_level: if True, every character will be treated as a token.\n",
    "        oov_token: if given, it will be added to word_index and used to\n",
    "            replace out-of-vocabulary words during text_to_sequence calls\n",
    "    By default, all punctuation is removed, turning the texts into\n",
    "    space-separated sequences of words\n",
    "    (words maybe include the `'` character). These sequences are then\n",
    "    split into lists of tokens. They will then be indexed or vectorized.\n",
    "    `0` is a reserved index that won't be assigned to any word.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_words=None,\n",
    "                 filters='!\"#$%&()*+，。；,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                 lower=True,\n",
    "                 split=' ',\n",
    "                 char_level=False,\n",
    "                 oov_token=None,\n",
    "                 document_count=0,\n",
    "                 **kwargs):\n",
    "        # Legacy support\n",
    "        if 'nb_words' in kwargs:\n",
    "            warnings.warn('The `nb_words` argument in `Tokenizer` '\n",
    "                          'has been renamed `num_words`.')\n",
    "            num_words = kwargs.pop('nb_words')\n",
    "        if kwargs:\n",
    "            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n",
    "\n",
    "        self.word_counts = OrderedDict()\n",
    "        self.word_docs = defaultdict(int)\n",
    "        self.filters = filters\n",
    "        self.split = split\n",
    "        self.lower = lower\n",
    "        self.num_words = num_words\n",
    "        self.document_count = document_count\n",
    "        self.char_level = char_level\n",
    "        self.oov_token = oov_token\n",
    "        self.index_docs = defaultdict(int)\n",
    "        self.word_index = dict()\n",
    "        self.index_word = dict()\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        \"\"\"Updates internal vocabulary based on a list of texts.\n",
    "        In the case where texts contains lists,\n",
    "        we assume each entry of the lists to be a token.\n",
    "        Required before using `texts_to_sequences` or `texts_to_matrix`.\n",
    "        # Arguments\n",
    "            texts: can be a list of strings,\n",
    "                a generator of strings (for memory-efficiency),\n",
    "                or a list of list of strings.\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            self.document_count += 1\n",
    "            if self.char_level or isinstance(text, list):\n",
    "                if self.lower:\n",
    "                    if isinstance(text, list):\n",
    "                        text = [text_elem.lower() for text_elem in text]\n",
    "                    else:\n",
    "                        text = text.lower()\n",
    "                seq = text\n",
    "            else:\n",
    "                seq = text_to_word_sequence(text,\n",
    "                                            self.filters,\n",
    "                                            self.lower,\n",
    "                                            self.split)\n",
    "            for w in seq:\n",
    "                if w in self.word_counts:\n",
    "                    self.word_counts[w] += 1\n",
    "                else:\n",
    "                    self.word_counts[w] = 1\n",
    "            for w in set(seq):\n",
    "                # In how many documents each word occurs\n",
    "                self.word_docs[w] += 1\n",
    "\n",
    "        wcounts = list(self.word_counts.items())\n",
    "        wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "        # forcing the oov_token to index 1 if it exists\n",
    "        if self.oov_token is None:\n",
    "            sorted_voc = []\n",
    "        else:\n",
    "            sorted_voc = [self.oov_token]\n",
    "        sorted_voc.extend(wc[0] for wc in wcounts)\n",
    "\n",
    "        # note that index 0 is reserved, never assigned to an existing word\n",
    "        self.word_index = dict(\n",
    "            list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))))\n",
    "\n",
    "        self.index_word = dict((c, w) for w, c in self.word_index.items())\n",
    "\n",
    "        for w, c in list(self.word_docs.items()):\n",
    "            self.index_docs[self.word_index[w]] = c\n",
    "            \n",
    "    def fit_on_cntexts(self, texts):\n",
    "        '''\n",
    "        假设输入是一组中文列表\n",
    "        '''\n",
    "        for text in texts:\n",
    "            self.document_count += 1\n",
    "\n",
    "            if isinstance(text, list):\n",
    "                longtext = ' '.join(text)           \n",
    "                text = longtext\n",
    "            seq = cntext_to_word_sequence(text, self.filters)\n",
    "            for w in seq:\n",
    "                if w in self.word_counts:\n",
    "                    self.word_counts[w] += 1\n",
    "                else:\n",
    "                    self.word_counts[w] = 1\n",
    "            for w in set(seq):\n",
    "            # In how many documents each word occurs\n",
    "               self.word_docs[w] += 1\n",
    "\n",
    "        wcounts = list(self.word_counts.items())\n",
    "        wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "        # forcing the oov_token to index 1 if it exists        \n",
    "        if self.oov_token is None:\n",
    "            sorted_voc = []\n",
    "        else:\n",
    "            sorted_voc = [self.oov_token]\n",
    "        sorted_voc.extend(wc[0] for wc in wcounts)\n",
    "\n",
    "        # note that index 0 is reserved, never assigned to an existing word\n",
    "        self.word_index = dict(\n",
    "                list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1))))\n",
    "        )\n",
    "\n",
    "        self.index_word = dict((c, w) for w, c in self.word_index.items())\n",
    "\n",
    "        for w, c in list(self.word_docs.items()):\n",
    "            self.index_docs[self.word_index[w]] = c\n",
    "\n",
    "    def fit_on_sequences(self, sequences):\n",
    "        \"\"\"Updates internal vocabulary based on a list of sequences.\n",
    "        Required before using `sequences_to_matrix`\n",
    "        (if `fit_on_texts` was never called).\n",
    "        # Arguments\n",
    "            sequences: A list of sequence.\n",
    "                A \"sequence\" is a list of integer word indices.\n",
    "        \"\"\"\n",
    "        self.document_count += len(sequences)\n",
    "        for seq in sequences:\n",
    "            seq = set(seq)\n",
    "            for i in seq:\n",
    "                self.index_docs[i] += 1\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"Transforms each text in texts to a sequence of integers.\n",
    "        Only top `num_words-1` most frequent words will be taken into account.\n",
    "        Only words known by the tokenizer will be taken into account.\n",
    "        # Arguments\n",
    "            texts: A list of texts (strings).\n",
    "        # Returns\n",
    "            A list of sequences.\n",
    "        \"\"\"\n",
    "        return list(self.texts_to_sequences_generator(texts))\n",
    "    \n",
    "    def cntexts_to_sequences(self, texts):\n",
    "        \"\"\"Transforms each text in texts to a sequence of integers.\n",
    "        Only top `num_words-1` most frequent words will be taken into account.\n",
    "        Only words known by the tokenizer will be taken into account.\n",
    "        # Arguments\n",
    "            texts: A list of texts (strings).\n",
    "        # Returns\n",
    "            A list of sequences.\n",
    "        \"\"\"\n",
    "        return list(self.cntexts_to_sequences_generator(texts))    \n",
    "\n",
    "    def texts_to_sequences_generator(self, texts):\n",
    "        \"\"\"Transforms each text in `texts` to a sequence of integers.\n",
    "        Each item in texts can also be a list,\n",
    "        in which case we assume each item of that list to be a token.\n",
    "        Only top `num_words-1` most frequent words will be taken into account.\n",
    "        Only words known by the tokenizer will be taken into account.\n",
    "        # Arguments\n",
    "            texts: A list of texts (strings).\n",
    "        # Yields\n",
    "            Yields individual sequences.\n",
    "        \"\"\"\n",
    "        num_words = self.num_words\n",
    "        oov_token_index = self.word_index.get(self.oov_token)\n",
    "        \n",
    "        for text in texts:\n",
    "            if self.char_level or isinstance(text, list):\n",
    "                if self.lower:\n",
    "                    if isinstance(text, list):\n",
    "                        text = [text_elem.lower() for text_elem in text]\n",
    "                    else:\n",
    "                        text = text.lower()\n",
    "                seq = text\n",
    "            else:\n",
    "                seq = text_to_word_sequence(text,\n",
    "                                            self.filters,\n",
    "                                            self.lower,\n",
    "                                            self.split)\n",
    "            vect = []\n",
    "            for w in seq:\n",
    "                i = self.word_index.get(w)\n",
    "                if i is not None:\n",
    "                    if num_words and i >= num_words:\n",
    "                        if oov_token_index is not None:\n",
    "                            vect.append(oov_token_index)\n",
    "                    else:\n",
    "                        vect.append(i)\n",
    "                elif self.oov_token is not None:\n",
    "                    vect.append(oov_token_index)\n",
    "            yield vect\n",
    "            \n",
    "    def cntexts_to_sequences_generator(self, texts):\n",
    "        \"\"\"Transforms each text in `texts` to a sequence of integers.\n",
    "        Each item in texts can also be a list,\n",
    "        in which case we assume each item of that list to be a token.\n",
    "        Only top `num_words-1` most frequent words will be taken into account.\n",
    "        Only words known by the tokenizer will be taken into account.\n",
    "        # Arguments\n",
    "            texts: A list of texts (strings).\n",
    "        # Yields\n",
    "            Yields individual sequences.\n",
    "        \"\"\"\n",
    "        num_words = self.num_words\n",
    "        oov_token_index = self.word_index.get(self.oov_token)\n",
    "        for text in texts:\n",
    "            self.document_count += 1\n",
    "\n",
    "            if isinstance(text, list):\n",
    "                longtext = ' '.join(text)           \n",
    "                text = longtext\n",
    "            seq = cntext_to_word_sequence(text, self.filters)\n",
    "                    \n",
    "            vect = []\n",
    "            for w in seq:\n",
    "                i = self.word_index.get(w)\n",
    "                if i is not None:\n",
    "                    if num_words and i >= num_words:\n",
    "                        if oov_token_index is not None:\n",
    "                            vect.append(oov_token_index)\n",
    "                    else:\n",
    "                        vect.append(i)\n",
    "                elif self.oov_token is not None:\n",
    "                    vect.append(oov_token_index)\n",
    "            yield vect            \n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        \"\"\"Transforms each sequence into a list of text.\n",
    "        Only top `num_words-1` most frequent words will be taken into account.\n",
    "        Only words known by the tokenizer will be taken into account.\n",
    "        # Arguments\n",
    "            texts: A list of sequences (list of integers).\n",
    "        # Returns\n",
    "            A list of texts (strings)\n",
    "        \"\"\"\n",
    "        return list(self.sequences_to_texts_generator(sequences))\n",
    "\n",
    "    def sequences_to_texts_generator(self, sequences):\n",
    "        \"\"\"Transforms each sequence in `sequences` to a list of texts(strings).\n",
    "        Each sequence has to a list of integers.\n",
    "        In other words, sequences should be a list of sequences\n",
    "        Only top `num_words-1` most frequent words will be taken into account.\n",
    "        Only words known by the tokenizer will be taken into account.\n",
    "        # Arguments\n",
    "            texts: A list of sequences.\n",
    "        # Yields\n",
    "            Yields individual texts.\n",
    "        \"\"\"\n",
    "        num_words = self.num_words\n",
    "        oov_token_index = self.word_index.get(self.oov_token)\n",
    "        for seq in sequences:\n",
    "            vect = []\n",
    "            for num in seq:\n",
    "                word = self.index_word.get(num)\n",
    "                if word is not None:\n",
    "                    if num_words and num >= num_words:\n",
    "                        if oov_token_index is not None:\n",
    "                            vect.append(self.index_word[oov_token_index])\n",
    "                    else:\n",
    "                        vect.append(word)\n",
    "                elif self.oov_token is not None:\n",
    "                    vect.append(self.index_word[oov_token_index])\n",
    "            vect = ' '.join(vect)\n",
    "            yield vect\n",
    "\n",
    "    def texts_to_matrix(self, texts, mode='binary'):\n",
    "        \"\"\"Convert a list of texts to a Numpy matrix.\n",
    "        # Arguments\n",
    "            texts: list of strings.\n",
    "            mode: one of \"binary\", \"count\", \"tfidf\", \"freq\".\n",
    "        # Returns\n",
    "            A Numpy matrix.\n",
    "        \"\"\"\n",
    "        sequences = self.texts_to_sequences(texts)\n",
    "        return self.sequences_to_matrix(sequences, mode=mode)\n",
    "\n",
    "    def sequences_to_matrix(self, sequences, mode='binary'):\n",
    "        \"\"\"Converts a list of sequences into a Numpy matrix.\n",
    "        # Arguments\n",
    "            sequences: list of sequences\n",
    "                (a sequence is a list of integer word indices).\n",
    "            mode: one of \"binary\", \"count\", \"tfidf\", \"freq\"\n",
    "        # Returns\n",
    "            A Numpy matrix.\n",
    "        # Raises\n",
    "            ValueError: In case of invalid `mode` argument,\n",
    "                or if the Tokenizer requires to be fit to sample data.\n",
    "        \"\"\"\n",
    "        if not self.num_words:\n",
    "            if self.word_index:\n",
    "                num_words = len(self.word_index) + 1\n",
    "            else:\n",
    "                raise ValueError('Specify a dimension (`num_words` argument), '\n",
    "                                 'or fit on some text data first.')\n",
    "        else:\n",
    "            num_words = self.num_words\n",
    "\n",
    "        if mode == 'tfidf' and not self.document_count:\n",
    "            raise ValueError('Fit the Tokenizer on some data '\n",
    "                             'before using tfidf mode.')\n",
    "\n",
    "        x = np.zeros((len(sequences), num_words))\n",
    "        for i, seq in enumerate(sequences):\n",
    "            if not seq:\n",
    "                continue\n",
    "            counts = defaultdict(int)\n",
    "            for j in seq:\n",
    "                if j >= num_words:\n",
    "                    continue\n",
    "                counts[j] += 1\n",
    "            for j, c in list(counts.items()):\n",
    "                if mode == 'count':\n",
    "                    x[i][j] = c\n",
    "                elif mode == 'freq':\n",
    "                    x[i][j] = c / len(seq)\n",
    "                elif mode == 'binary':\n",
    "                    x[i][j] = 1\n",
    "                elif mode == 'tfidf':\n",
    "                    # Use weighting scheme 2 in\n",
    "                    # https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "                    tf = 1 + np.log(c)\n",
    "                    idf = np.log(1 + self.document_count /\n",
    "                                 (1 + self.index_docs.get(j, 0)))\n",
    "                    x[i][j] = tf * idf\n",
    "                else:\n",
    "                    raise ValueError('Unknown vectorization mode:', mode)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        '''Returns the tokenizer configuration as Python dictionary.\n",
    "        The word count dictionaries used by the tokenizer get serialized\n",
    "        into plain JSON, so that the configuration can be read by other\n",
    "        projects.\n",
    "        # Returns\n",
    "            A Python dictionary with the tokenizer configuration.\n",
    "        '''\n",
    "        json_word_counts = json.dumps(self.word_counts)\n",
    "        json_word_docs = json.dumps(self.word_docs)\n",
    "        json_index_docs = json.dumps(self.index_docs)\n",
    "        json_word_index = json.dumps(self.word_index)\n",
    "        json_index_word = json.dumps(self.index_word)\n",
    "\n",
    "        return {\n",
    "            'num_words': self.num_words,\n",
    "            'filters': self.filters,\n",
    "            'lower': self.lower,\n",
    "            'split': self.split,\n",
    "            'char_level': self.char_level,\n",
    "            'oov_token': self.oov_token,\n",
    "            'document_count': self.document_count,\n",
    "            'word_counts': json_word_counts,\n",
    "            'word_docs': json_word_docs,\n",
    "            'index_docs': json_index_docs,\n",
    "            'index_word': json_index_word,\n",
    "            'word_index': json_word_index\n",
    "        }\n",
    "\n",
    "    def to_json(self, **kwargs):\n",
    "        \"\"\"Returns a JSON string containing the tokenizer configuration.\n",
    "        To load a tokenizer from a JSON string, use\n",
    "        `keras.preprocessing.text.tokenizer_from_json(json_string)`.\n",
    "        # Arguments\n",
    "            **kwargs: Additional keyword arguments\n",
    "                to be passed to `json.dumps()`.\n",
    "        # Returns\n",
    "            A JSON string containing the tokenizer configuration.\n",
    "        \"\"\"\n",
    "        config = self.get_config()\n",
    "        tokenizer_config = {\n",
    "            'class_name': self.__class__.__name__,\n",
    "            'config': config\n",
    "        }\n",
    "        return json.dumps(tokenizer_config, **kwargs)\n",
    "\n",
    "\n",
    "def tokenizer_from_json(json_string):\n",
    "    \"\"\"Parses a JSON tokenizer configuration file and returns a\n",
    "    tokenizer instance.\n",
    "    # Arguments\n",
    "        json_string: JSON string encoding a tokenizer configuration.\n",
    "    # Returns\n",
    "        A Keras Tokenizer instance\n",
    "    \"\"\"\n",
    "    tokenizer_config = json.loads(json_string)\n",
    "    config = tokenizer_config.get('config')\n",
    "\n",
    "    word_counts = json.loads(config.pop('word_counts'))\n",
    "    word_docs = json.loads(config.pop('word_docs'))\n",
    "    index_docs = json.loads(config.pop('index_docs'))\n",
    "    # Integer indexing gets converted to strings with json.dumps()\n",
    "    index_docs = {int(k): v for k, v in index_docs.items()}\n",
    "    index_word = json.loads(config.pop('index_word'))\n",
    "    index_word = {int(k): v for k, v in index_word.items()}\n",
    "    word_index = json.loads(config.pop('word_index'))\n",
    "\n",
    "    tokenizer = Tokenizer(**config)\n",
    "    tokenizer.word_counts = word_counts\n",
    "    tokenizer.word_docs = word_docs\n",
    "    tokenizer.index_docs = index_docs\n",
    "    tokenizer.word_index = word_index\n",
    "    tokenizer.index_word = index_word\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "def cntext_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',):\n",
    "    translate_dict = dict((c, ' ') for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    words = jieba.cut(text)\n",
    "    seq = [w for w in words if w]\n",
    "    return seq\n",
    "\n",
    "    \n",
    "def cntexts_to_sequences(texts, num_words, oov_token, document_count, filters, word_index ):\n",
    "    \"\"\"Transforms each text in texts to a sequence of integers.\n",
    "    Only top `num_words-1` most frequent words will be taken into account.\n",
    "    Only words known by the tokenizer will be taken into account.\n",
    "    # Arguments\n",
    "        texts: A list of texts (strings).\n",
    "    # Returns\n",
    "        A list of sequences.\n",
    "    \"\"\"\n",
    "    return list(cntexts_to_sequences_generator(texts, num_words, oov_token, document_count, filters, word_index))    \n",
    "\n",
    "\n",
    "def cntexts_to_sequences_generator(texts, num_words, oov_token, document_count, filters, word_index ):\n",
    "    \"\"\"Transforms each Chinese text in `texts` to a sequence of integers.\n",
    "    Each item in texts can also be a list,\n",
    "    in which case we assume each item of that list to be a token.\n",
    "    Only top `num_words-1` most frequent words will be taken into account.\n",
    "    Only words known by the tokenizer will be taken into account.\n",
    "    # Arguments\n",
    "        texts: A list of texts (strings).\n",
    "    # Yields\n",
    "        Yields individual sequences.\n",
    "    \"\"\"\n",
    "    num_words = num_words\n",
    "    oov_token_index = word_index.get(oov_token)\n",
    "    for text in texts:\n",
    "        document_count += 1\n",
    "        #print(text)\n",
    "\n",
    "        if isinstance(text, list):\n",
    "            longtext = ' '.join(text)           \n",
    "            text = longtext\n",
    "        seq = cntext_to_word_sequence(text, filters)\n",
    "        #print(seq, '\\n')\n",
    "                \n",
    "        vect = []\n",
    "        temp = []\n",
    "        for w in seq:\n",
    "            i = word_index.get(w)\n",
    "            temp.append(i)\n",
    "            if i is not None:\n",
    "                if num_words and i >= num_words:\n",
    "                    if oov_token_index is not None:\n",
    "                        vect.append(oov_token_index)\n",
    "                else:\n",
    "                    vect.append(i)\n",
    "            elif oov_token is not None:\n",
    "                vect.append(oov_token_index)\n",
    "        #print(temp)\n",
    "        #print('-------------')\n",
    "        yield vect       \n",
    "\n",
    "        \n",
    "class cnTokenizer(Tokenizer):\n",
    "    \"\"\"Text tokenization utility class.\n",
    "    This class allows to vectorize a text corpus, by turning each\n",
    "    text into either a sequence of integers (each integer being the index\n",
    "    of a token in a dictionary) or into a vector where the coefficient\n",
    "    for each token could be binary, based on word count, based on tf-idf...\n",
    "    # Arguments\n",
    "        num_words: the maximum number of words to keep, based\n",
    "            on word frequency. Only the most common `num_words-1` words will\n",
    "            be kept.\n",
    "        filters: a string where each element is a character that will be\n",
    "            filtered from the texts. The default is all punctuation, plus\n",
    "            tabs and line breaks, minus the `'` character.\n",
    "        lower: boolean. Whether to convert the texts to lowercase.\n",
    "        split: str. Separator for word splitting.\n",
    "        char_level: if True, every character will be treated as a token.\n",
    "        oov_token: if given, it will be added to word_index and used to\n",
    "            replace out-of-vocabulary words during text_to_sequence calls\n",
    "    By default, all punctuation is removed, turning the texts into\n",
    "    space-separated sequences of words\n",
    "    (words maybe include the `'` character). These sequences are then\n",
    "    split into lists of tokens. They will then be indexed or vectorized.\n",
    "    `0` is a reserved index that won't be assigned to any word.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_words=None,\n",
    "                 filters='!\"#$%&()*+，。；,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                 lower=True,\n",
    "                 split=' ',\n",
    "                 char_level=False,\n",
    "                 oov_token=None,\n",
    "                 document_count=0,\n",
    "                 **kwargs):\n",
    "        # Legacy support\n",
    "        if 'nb_words' in kwargs:\n",
    "            warnings.warn('The `nb_words` argument in `Tokenizer` '\n",
    "                          'has been renamed `num_words`.')\n",
    "            num_words = kwargs.pop('nb_words')\n",
    "        if kwargs:\n",
    "            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n",
    "\n",
    "        self.word_counts = OrderedDict()\n",
    "        self.word_docs = defaultdict(int)\n",
    "        self.filters = filters\n",
    "        self.split = split\n",
    "        self.lower = lower\n",
    "        self.num_words = num_words\n",
    "        self.document_count = document_count\n",
    "        self.char_level = char_level\n",
    "        self.oov_token = oov_token\n",
    "        self.index_docs = defaultdict(int)\n",
    "        self.word_index = dict()\n",
    "        self.index_word = dict()\n",
    "            \n",
    "    def fit_on_cntexts(self, texts):\n",
    "        '''\n",
    "        假设输入是一组中文列表\n",
    "        '''\n",
    "        for text in texts:\n",
    "            self.document_count += 1\n",
    "\n",
    "            if isinstance(text, list):\n",
    "                longtext = ' '.join(text)           \n",
    "                text = longtext\n",
    "            seq = cntext_to_word_sequence(text, self.filters)\n",
    "            for w in seq:\n",
    "                if w in self.word_counts:\n",
    "                    self.word_counts[w] += 1\n",
    "                else:\n",
    "                    self.word_counts[w] = 1\n",
    "            for w in set(seq):\n",
    "            # In how many documents each word occurs\n",
    "               self.word_docs[w] += 1\n",
    "\n",
    "        wcounts = list(self.word_counts.items())\n",
    "        wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "        # forcing the oov_token to index 1 if it exists        \n",
    "        if self.oov_token is None:\n",
    "            sorted_voc = []\n",
    "        else:\n",
    "            sorted_voc = [self.oov_token]\n",
    "        sorted_voc.extend(wc[0] for wc in wcounts)\n",
    "\n",
    "        # note that index 0 is reserved, never assigned to an existing word\n",
    "        self.word_index = dict(\n",
    "                list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1))))\n",
    "        )\n",
    "\n",
    "        self.index_word = dict((c, w) for w, c in self.word_index.items())\n",
    "\n",
    "        for w, c in list(self.word_docs.items()):\n",
    "            self.index_docs[self.word_index[w]] = c\n",
    "\n",
    "    \n",
    "    def cntexts_to_sequences(self, texts):\n",
    "        \"\"\"Transforms each text in texts to a sequence of integers.\n",
    "        Only top `num_words-1` most frequent words will be taken into account.\n",
    "        Only words known by the tokenizer will be taken into account.\n",
    "        # Arguments\n",
    "            texts: A list of texts (strings).\n",
    "        # Returns\n",
    "            A list of sequences.\n",
    "        \"\"\"\n",
    "        return list(self.cntexts_to_sequences_generator(texts))    \n",
    "\n",
    "    def cntexts_to_sequences_generator(self, texts):\n",
    "        \"\"\"Transforms each text in `texts` to a sequence of integers.\n",
    "        Each item in texts can also be a list,\n",
    "        in which case we assume each item of that list to be a token.\n",
    "        Only top `num_words-1` most frequent words will be taken into account.\n",
    "        Only words known by the tokenizer will be taken into account.\n",
    "        # Arguments\n",
    "            texts: A list of texts (strings).\n",
    "        # Yields\n",
    "            Yields individual sequences.\n",
    "        \"\"\"\n",
    "        num_words = self.num_words\n",
    "        oov_token_index = self.word_index.get(self.oov_token)\n",
    "        for text in texts:\n",
    "            self.document_count += 1\n",
    "\n",
    "            if isinstance(text, list):\n",
    "                longtext = ' '.join(text)           \n",
    "                text = longtext\n",
    "            seq = cntext_to_word_sequence(text, self.filters)\n",
    "                    \n",
    "            vect = []\n",
    "            for w in seq:\n",
    "                i = self.word_index.get(w)\n",
    "                if i is not None:\n",
    "                    if num_words and i >= num_words:\n",
    "                        if oov_token_index is not None:\n",
    "                            vect.append(oov_token_index)\n",
    "                    else:\n",
    "                        vect.append(i)\n",
    "                elif self.oov_token is not None:\n",
    "                    vect.append(oov_token_index)\n",
    "            yield vect            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们实验一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 3, 20: 1, 11: 2, 9: 2, 5: 2, 4: 2, 8: 2, 10: 2, 3: 2, 18: 1, 23: 1, 2: 3, 14: 1, 17: 1, 13: 1, 21: 1, 22: 1, 16: 1, 7: 2, 12: 1, 19: 1, 15: 1, 6: 2, 28: 1, 33: 1, 34: 1, 31: 1, 32: 1, 27: 1, 30: 1, 29: 1, 25: 1, 24: 1, 26: 1, 35: 1, 37: 1, 36: 1, 40: 1, 41: 1, 38: 1, 43: 1, 39: 1, 42: 1}) \n",
      "\n",
      "---------------\n",
      "{'的': 1, '单词': 2, '独热': 3, '编码': 4, ' ': 5, '当前': 6, '单词表': 7, '个': 8, '简单': 9, '有': 10, '和': 11, '是': 12, '对于': 13, '中': 14, '使用': 15, '1': 16, '向量': 17, '进行': 18, '表达': 19, '方式': 20, '自身': 21, '缺点': 22, '有点': 23, '研究者': 24, '探索': 25, '了': 26, '可以': 27, '解决': 28, '存储': 29, '效率': 30, '之间': 31, '关系': 32, '问题': 33, '方法': 34, '虽然': 35, '假如': 36, '10': 37, '不同': 38, '并且': 39, '每个': 40, '都': 41, '不': 42, '一样': 43} \n",
      "\n",
      "{1: '的', 2: '单词', 3: '独热', 4: '编码', 5: ' ', 6: '当前', 7: '单词表', 8: '个', 9: '简单', 10: '有', 11: '和', 12: '是', 13: '对于', 14: '中', 15: '使用', 16: '1', 17: '向量', 18: '进行', 19: '表达', 20: '方式', 21: '自身', 22: '缺点', 23: '有点', 24: '研究者', 25: '探索', 26: '了', 27: '可以', 28: '解决', 29: '存储', 30: '效率', 31: '之间', 32: '关系', 33: '问题', 34: '方法', 35: '虽然', 36: '假如', 37: '10', 38: '不同', 39: '并且', 40: '每个', 41: '都', 42: '不', 43: '一样'}\n",
      "defaultdict(<class 'int'>, {'的': 3, '方式': 1, '和': 2, '简单': 2, ' ': 2, '编码': 2, '个': 2, '有': 2, '独热': 2, '进行': 1, '有点': 1, '单词': 3, '中': 1, '向量': 1, '对于': 1, '自身': 1, '缺点': 1, '1': 1, '单词表': 2, '是': 1, '表达': 1, '使用': 1, '当前': 2, '解决': 1, '问题': 1, '方法': 1, '之间': 1, '关系': 1, '可以': 1, '效率': 1, '存储': 1, '探索': 1, '研究者': 1, '了': 1, '虽然': 1, '10': 1, '假如': 1, '每个': 1, '都': 1, '不同': 1, '一样': 1, '并且': 1, '不': 1})\n",
      "defaultdict(<class 'int'>, {1: 3, 20: 1, 11: 2, 9: 2, 5: 2, 4: 2, 8: 2, 10: 2, 3: 2, 18: 1, 23: 1, 2: 3, 14: 1, 17: 1, 13: 1, 21: 1, 22: 1, 16: 1, 7: 2, 12: 1, 19: 1, 15: 1, 6: 2, 28: 1, 33: 1, 34: 1, 31: 1, 32: 1, 27: 1, 30: 1, 29: 1, 25: 1, 24: 1, 26: 1, 35: 1, 37: 1, 36: 1, 40: 1, 41: 1, 38: 1, 43: 1, 39: 1, 42: 1})\n",
      "OrderedDict([('独热', 3), ('编码', 3), ('是', 1), ('对于', 1), ('当前', 2), ('单词表', 2), ('中', 1), ('的', 6), ('单词', 4), ('使用', 1), ('1', 1), ('个', 2), ('向量', 1), ('进行', 1), ('表达', 1), ('简单', 2), ('方式', 1), (' ', 3), ('有', 2), ('自身', 1), ('缺点', 1), ('和', 2), ('有点', 1), ('研究者', 1), ('探索', 1), ('了', 1), ('可以', 1), ('解决', 1), ('存储', 1), ('效率', 1), ('之间', 1), ('关系', 1), ('问题', 1), ('方法', 1), ('虽然', 1), ('假如', 1), ('10', 1), ('不同', 1), ('并且', 1), ('每个', 1), ('都', 1), ('不', 1), ('一样', 1)])\n",
      "\n",
      "\n",
      "[[3, 4, 12, 13, 6, 7, 14, 1, 2, 15, 16, 8, 17, 18, 19, 1, 9, 20, 5, 3, 4, 10, 21, 1, 22, 11, 23, 5], [24, 25, 26, 27, 28, 29, 30, 11, 2, 31, 32, 33, 1, 34], [3, 4, 35, 9], [36, 6, 1, 7, 10, 37, 8, 38, 1, 2, 5, 39, 40, 2, 41, 42, 43]]\n"
     ]
    }
   ],
   "source": [
    "documents = [u'独热编码是对于当前单词表中的单词使用1个向量进行表达的简单方式，独热编码有自身的缺点和有点。', \n",
    "             u'研究者探索了可以解决存储效率和单词之间关系问题的方法', \n",
    "             u'独热编码虽然简单', \n",
    "             u'假如当前的单词表有10个不同的单词，并且每个单词都不一样']\n",
    "\n",
    "corpus = documents\n",
    "tokenizer = cnTokenizer()\n",
    "tokenizer.fit_on_cntexts(documents)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "text_sequences=tokenizer.cntexts_to_sequences(documents)\n",
    "#seq, word_index, index_word, word_docs, index_docs, word_counts, document_count = fit_on_cntexts(corpus)\n",
    "print(tokenizer.index_docs,'\\n')\n",
    "print('---------------')\n",
    "print(tokenizer.word_index, '\\n')\n",
    "print(tokenizer.index_word)\n",
    "print(tokenizer.word_docs)\n",
    "print(tokenizer.index_docs)\n",
    "print(tokenizer.word_counts)\n",
    "print('\\n')\n",
    "print(text_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经对keras的标准标注类做了必要的小修改，使得其能顺利处理中文文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW的keras实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们在keras中来实现基于CBOW算法的word2vec模型。我们需要进行如下的设计：\n",
    "1. 首先我们要将原始输入的文字进行标注化（tokenize），下标代表每一个单词，构造单词表；\n",
    "2. 构造一个CBOW数据生成器，能产出（上下文，目标单词）的组合\n",
    "3. 构造CBOW的网络结构\n",
    "4. 训练CBOW模型\n",
    "5. 获取词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 1\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "# build vocabulary of unique words\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 # context window size\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 2\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)\n",
    "            \n",
    "            \n",
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-a1e7695227b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# build CBOW architecture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcbow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "# 步骤 3\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "# build CBOW architecture\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# view model summary\n",
    "print(cbow.summary())\n",
    "\n",
    "# visualize model structure\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n",
    "rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/Chapter1-CBOW-model.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 4\n",
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-f0d932a0a87f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 步骤 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 步骤 5\n",
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-aadac1bded9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'accuracy'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import accuracy\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Lambda, Dense, merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import SGD\n",
    "from keras.objectives import mse\n",
    "\n",
    "import global_settings as G\n",
    "from sentences_generator import Sentences\n",
    "import vocab_generator as V_gen\n",
    "import save_embeddings as S\n",
    "\n",
    "k = G.window_size # context windows size\n",
    "context_size = 2*k\n",
    "\n",
    "# Creating a sentence generator from demo file\n",
    "sentences = Sentences(\"test_file.txt\")\n",
    "vocabulary = dict()\n",
    "V_gen.build_vocabulary(vocabulary, sentences)\n",
    "V_gen.filter_vocabulary_based_on(vocabulary, G.min_count)\n",
    "reverse_vocabulary = V_gen.generate_inverse_vocabulary_lookup(vocabulary, \"vocab.txt\")\n",
    "\n",
    "# generate embedding matrix with all values between -1/2d, 1/2d\n",
    "embedding = np.random.uniform(-1.0/2.0/G.embedding_dimension, 1.0/2.0/G.embedding_dimension, (G.vocab_size+3, G.embedding_dimension))\n",
    "\n",
    "# Creating CBOW model\n",
    "# Model has 3 inputs\n",
    "# Current word index, context words indexes and negative sampled word indexes\n",
    "word_index = Input(shape=(1,))\n",
    "context = Input(shape=(context_size,))\n",
    "negative_samples = Input(shape=(G.negative,))\n",
    "# All the inputs are processed through a common embedding layer\n",
    "shared_embedding_layer = Embedding(input_dim=(G.vocab_size+3), output_dim=G.embedding_dimension, weights=[embedding])\n",
    "word_embedding = shared_embedding_layer(word_index)\n",
    "context_embeddings = shared_embedding_layer(context)\n",
    "negative_words_embedding = shared_embedding_layer(negative_samples)\n",
    "# Now the context words are averaged to get the CBOW vector\n",
    "cbow = Lambda(lambda x: K.mean(x, axis=1), output_shape=(G.embedding_dimension,))(context_embeddings)\n",
    "# The context is multiplied (dot product) with current word and negative sampled words\n",
    "word_context_product = merge([word_embedding, cbow], mode='dot')\n",
    "negative_context_product = merge([negative_words_embedding, cbow], mode='dot', concat_axis=-1)\n",
    "# The dot products are outputted\n",
    "model = Model(input=[word_index, context, negative_samples], output=[word_context_product, negative_context_product])\n",
    "# binary crossentropy is applied on the output\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "print(model.summary())\n",
    "\n",
    "# model.fit_generator(V_gen.pretraining_batch_generator(sentences, vocabulary, reverse_vocabulary), samples_per_epoch=G.train_words, nb_epoch=1)\n",
    "model.fit_generator(V_gen.pretraining_batch_generator(sentences, vocabulary, reverse_vocabulary), samples_per_epoch=10, nb_epoch=1)\n",
    "# Save the trained embedding\n",
    "S.save_embeddings(\"embedding.txt\", shared_embedding_layer.get_weights()[0], vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram模型的keras实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Merge, Reshape, Activation, Flatten, Input, merge, Dense\n",
    "from keras.layers.core import Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.visualize_util import model_to_dot, plot\n",
    "from keras.preprocessing.text import Tokenizer, base_filter\n",
    "from keras.preprocessing.sequence import skipgrams, pad_sequences\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from gensim.models.doc2vec import Word2Vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence for sentence in data2['text'] if sentence.count(\" \") >= 2]\n",
    "tokenizer = Tokenizer(filters=base_filter()+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "vocab = tokenizer.word_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 10\n",
    "dim_embeddings = 128\n",
    "inputs = Input(shape=(maxlen,), name = 'inputWord',dtype='int32')\n",
    "context = Input(shape=(maxlen,), name = 'contextWord',dtype='int32')\n",
    "\n",
    "embedded_sequences_input = Embedding(200000,\n",
    "                                     dim_embeddings,\n",
    "                                     input_length=maxlen,\n",
    "                                    name='input_embeddings',\n",
    "                                    trainable=True)(inputs)\n",
    "embedded_sequences_context = Embedding(200000,\n",
    "                                       dim_embeddings,\n",
    "                                       input_length=maxlen,\n",
    "                                       trainable=True,\n",
    "                                       name='context_embeddings')(context)\n",
    "\n",
    "embedded_sequences_context1 = Lambda(lambda s: K.sum(s, axis=1), output_shape=lambda s: (s[0],s[2]))(embedded_sequences_context)\n",
    "embedded_sequences_input1 = Lambda(lambda s: K.sum(s, axis=1), output_shape=lambda s: (s[0],s[2]))(embedded_sequences_input)\n",
    "\n",
    "# embedded_sequences_input1 = Reshape((1,), input_shape=(1,128))(embedded_sequences_input1)\n",
    "# embedded_sequences_context1 = Reshape((1,), input_shape=(1,128))(embedded_sequences_context1)\n",
    "\n",
    "final = merge([embedded_sequences_input1, embedded_sequences_context1], mode='dot', dot_axes=1)\n",
    "# final = Reshape((1,), input_shape=(1,1))(final)\n",
    "final = Dense(1, activation='sigmoid')(final)\n",
    "model = Model([inputs, context], final)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "display(SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence for sentence in data1['text'] if sentence.count(\" \") >= 2]\n",
    "tokenizer = Tokenizer(filters=base_filter()+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "maxlen = 10\n",
    "for _ in range(1):\n",
    "    loss = 0.\n",
    "    for doc in tokenizer.texts_to_sequences(data1['text']):\n",
    "#         print doc\n",
    "        data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=5, negative_samples=5.)\n",
    "        ngram_representation = []\n",
    "        ngram_contexts = []\n",
    "        ngram_targets = []\n",
    "        for j in data:\n",
    "            ngram_context_pairs = []\n",
    "            word1 = new_dict[j[0]]\n",
    "            word2 = new_dict[j[1]]\n",
    "            ngram_contexts.append(word1)\n",
    "            ngram_targets.append(word2)\n",
    "            ngram_context_pairs.append(word1)\n",
    "            ngram_context_pairs.append(word2)\n",
    "            ngram_representation.append(ngram_context_pairs)\n",
    "\n",
    "        ngram_contexts = pad_sequences(ngram_contexts, maxlen=10, dtype='int32')\n",
    "        ngram_targets = pad_sequences(ngram_targets,maxlen=10,dtype='int32')\n",
    "        X = [ngram_contexts,ngram_targets]\n",
    "#         print len(ngram_contexts[0])\n",
    "        Y = np.array(labels, dtype=np.int32)\n",
    "#         print ngram_contexts.shape, ngram_targets.shape, Y.shape\n",
    "        if ngram_contexts.shape[0]!=0:\n",
    "#             loss += model.train_on_batch(X,Y)\n",
    "            try:\n",
    "#                 print \"tried\"\n",
    "                loss += model.train_on_batch(X,Y)\n",
    "            except IndexError:\n",
    "                continue\n",
    "    print loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对计算出来的词向量进行可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  # pylint: disable=g-import-not-at-top\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "except ImportError:\n",
    "print('Please install sklearn, matplotlib, and scipy to show embeddings.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
